{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gridL_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-84c19d3ea16b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgridL_agents\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgridL_buffer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperience_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgridL_buffer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuffer_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gridL_agents'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from gridL_agents import agent\n",
    "from gridL_buffer import experience_buffer\n",
    "from gridL_buffer import buffer_image\n",
    "from gridL_game import gameData\n",
    "import threading\n",
    "path = \"./saved-models\"\n",
    "model_desc_file = \"saved_run_backlog.txt\"\n",
    "def time_string():\n",
    "\tt = time.gmtime()\n",
    "\ts = \"\"\n",
    "\ts = str(t[1]) + \".\" + str(t[2]) + \".\" + str(t[0]) + \"_\" + str(t[3]) + \":\" + str(t[4]) + \":\" + str(t[5])\n",
    "\treturn s\n",
    "def max(a,b):\n",
    "\tif a>b:\n",
    "\t\treturn a\n",
    "\treturn b\n",
    "def generate_frames(buffer, n_frames, n_threads):\n",
    "\tinit_cap = buffer.size()\n",
    "\tframes_per_thread = n_frames/n_threads\n",
    "\tthreads = []\n",
    "\tfor i in range (0, n_threads):\n",
    "\t\tif(i == 0):\n",
    "\t\t\tt = threading.Thread(target = gen_frames_worker, args = (buffer, frames_per_thread + (n_frames % n_threads),), daemon = True)\n",
    "\t\telse:\n",
    "\t\t\tt = threading.Thread(target = gen_frames_worker, args = (buffer, frames_per_thread,), daemon = True)\n",
    "\t\tthreads.append(t)\n",
    "\t\tt.start()\n",
    "\twhile buffer.size()-init_cap < n_frames:\n",
    "\t\ttime.sleep(1)\n",
    "\t\tprint(str(buffer.size()-init_cap) + \" rounds completed\")\n",
    "def gen_frames_worker(buffer, n_frames):\n",
    "\tgame = gameData(100)\n",
    "\tb = experience_buffer(n_frames)\n",
    "\tworker = agent(b, game, 1)\n",
    "\tfor i in range(0, int(n_frames)):\n",
    "\t\tworker.make_action(1)\n",
    "\tfor j in range (0, int(n_frames)):\n",
    "\t\tbuffer.add_frame(b.buffer[j])\n",
    "\n",
    "###### ^^^^^ VARIOUS STATIC METHODS AND VARIABLES ^^^^^ ######\n",
    "\t\t\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "while True:\t\n",
    "\ttf.reset_default_graph()\t\n",
    "\tgame = gameData(100)\n",
    "\tb = experience_buffer(1000000)\n",
    "\trandom_player = agent(experience_buffer(1000000), game, 1)\n",
    "\ttrainables = tf.trainable_variables()\n",
    "\ttrained_player = agent(b, game, 0.5)\n",
    "\tsaver = tf.train.Saver()\n",
    "\tload_model, train_model = 'n', 'y'\n",
    "\t#a = input(\"load model?\")\n",
    "\t#b = input(\"train model?\")\n",
    "\tif(load_model == 'y'):\n",
    "\t\t\tcpkt = tf.train.get_checkpoint_state(path)\n",
    "\t\t\tsaver.restore(trained_player.sess,cpkt.model_checkpoint_path)\n",
    "\tif(train_model == 'y'):\n",
    "\t\tinit_time = time.time()\n",
    "\t\t#for i in range (0, 20000):\n",
    "\t\t#\ttrained_player.make_action(1)\n",
    "\t\t#\tif((i+1)%1000 == 0):\n",
    "\t\t#\t\tprint(\"{} rounds completed\".format(i))\n",
    "\t\tgenerate_frames(b, 20000, 8)\n",
    "\t\tprint(\"generating frames took {} seconds\".format(time.time() - init_time))\n",
    "\t\tprint(b.buffer[int(b.size()/2)].o_s)\n",
    "\t\tfor k in range (0, 30):\n",
    "\t\t\tprint(\"{} epochs completed\".format(k))\n",
    "\t\t\ttrained_player.train(0, 10000, 1000)\n",
    "\tgame.reset_game()\n",
    "\trand_wins, trained_wins = 0,0\n",
    "\tnum_games = 100\n",
    "\tfor j in range (0, num_games):\n",
    "\t\tgame.reset_game()\n",
    "\t\tturn,c = 0,0\n",
    "\t\twhile game.is_over() == 0:\n",
    "\t\t\tif(turn == 0):\n",
    "\t\t\t\ttrained_player.make_action(0)\n",
    "\t\t\t\tturn = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\trandom_player.make_action(1)\n",
    "\t\t\t\tturn = 0\n",
    "\t\tif(turn == 1):\n",
    "\t\t\tif(game.is_over() == 2):\n",
    "\t\t\t\ttrained_wins += 1\n",
    "\t\t\t\tprint (\"Trained Win!\")\n",
    "\t\t\telse:\n",
    "\t\t\t\trand_wins += 1\n",
    "\t\t\t\tprint(\"Rand win\")\n",
    "\t\telse:\n",
    "\t\t\tif(game.is_over() == 2):\n",
    "\t\t\t\trand_wins += 1\n",
    "\t\t\t\tprint(\"Rand win\")\n",
    "\t\t\telse:\n",
    "\t\t\t\ttrained_wins += 1\n",
    "\t\t\t\tprint (\"Trained Win!\")\n",
    "\tprint(\"After {} games, the agent trained for 150,000 steps has a win ratio of {} to {} against a randomly playing agent\".format(num_games, trained_wins, rand_wins))\n",
    "\tdescription = \"400 thousand frames made randomly, 300 thousand trained randomly, updated training net every 1000 frames\"\n",
    "\tprint(description)\n",
    "\tprint(time_string())\n",
    "\tif(num_games-trained_wins < 10):\n",
    "\t\trun_name = 0.04\n",
    "\t\tsaver.save(trained_player.sess,path+'/model-'+str(run_name)+'.ckpt')\n",
    "\t\tprint(\"Saved Model\")\n",
    "\t\topp_name = \"Random\"\n",
    "\t\ts = \"\\n\\n\\n\\nName of Run: {}\\nName of Opponent: {}\\nDescription: {}\\nDate and Time of Run: {}\\nWins: {}\\nLosses: {}\\n\".format(run_name, opp_name, description, time_string(), trained_wins, rand_wins)\n",
    "\t\tf = open(model_desc_file, \"r\")\n",
    "\t\texisting_data = f.read()\n",
    "\t\tf.close()\n",
    "\t\tf = open(model_desc_file, \"w\")\n",
    "\t\tf.write(existing_data+s)\n",
    "\t\tf.close()\n",
    "\t\texit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
